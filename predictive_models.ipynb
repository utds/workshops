{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First let's import the necessary modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Specifying the Data Path\n",
    "\n",
    "cwd = os.getcwd()\n",
    "file_path = os.path.join(cwd, 'cleaned_speed_dating.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "female_df = df.loc[df['gender'] == 0]\n",
    "male_df = df.loc[df['gender'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_train, female_test = train_test_split(female_df, test_size = 0.2, random_state=42)\n",
    "male_train, male_test = train_test_split(male_df, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "lr = linear_model.LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set performance is 0.7276960327807785\n",
      "test set performance is 0.7297096053611318\n"
     ]
    }
   ],
   "source": [
    "predictors = ['attr_partner']\n",
    "lr_model = lr.fit(train[predictors].values, train['dec'].values)\n",
    "print('training set performance is {}'.format(lr_model.score(train[predictors].values, train['dec'].values)))\n",
    "print('test set performance is {}'.format(lr_model.score(test[predictors].values, test['dec'].values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, if you only use one variable, that is no different than just doing a cut off based on one of the previous graphs and make naive predictions based on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender',\n",
       " 'age',\n",
       " 'date',\n",
       " 'sports',\n",
       " 'tvsports',\n",
       " 'exercise',\n",
       " 'dining',\n",
       " 'museums',\n",
       " 'art',\n",
       " 'hiking',\n",
       " 'gaming',\n",
       " 'clubbing',\n",
       " 'reading',\n",
       " 'tv',\n",
       " 'theater',\n",
       " 'movies',\n",
       " 'concerts',\n",
       " 'music',\n",
       " 'shopping',\n",
       " 'yoga',\n",
       " 'attr_want',\n",
       " 'sinc_want',\n",
       " 'intel_want',\n",
       " 'fun_want',\n",
       " 'amb_want',\n",
       " 'shar_want',\n",
       " 'attr_self',\n",
       " 'sinc_self',\n",
       " 'fun_self',\n",
       " 'intel_self',\n",
       " 'amb_self',\n",
       " 'pid',\n",
       " 'age_partner',\n",
       " 'int_corr',\n",
       " 'samerace',\n",
       " 'attr_partner',\n",
       " 'sinc_partner',\n",
       " 'intel_partner',\n",
       " 'fun_partner',\n",
       " 'amb_partner',\n",
       " 'shar_partner',\n",
       " 'prob']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.columns[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set performance is 0.7135406965915441\n",
      "test set performance is 0.7073715562174236\n"
     ]
    }
   ],
   "source": [
    "predictors = ['age','date','int_corr',\n",
    " 'samerace',\n",
    " 'sinc_partner',\n",
    " 'intel_partner',\n",
    " 'fun_partner',\n",
    " 'amb_partner',\n",
    " 'shar_partner','prob']\n",
    "lr_model = lr.fit(train[predictors].values, train['dec'].values)\n",
    "print('training set performance is {}'.format(lr_model.score(train[predictors].values, train['dec'].values)))\n",
    "print('test set performance is {}'.format(lr_model.score(test[predictors].values, test['dec'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01254919 -0.01858074  0.03288736  0.06893665 -0.07068834  0.04977025\n",
      "   0.42423327 -0.15730692  0.258164    0.15273089]]\n"
     ]
    }
   ],
   "source": [
    "# You can also see how important each of those factors is (sort of)\n",
    "print(lr_model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it does seems like attractiveness is more indicative than anything else. Next you can try to combine them and repeat the same procedure. Is there an improvement to the performance? What can you infer from the coefficients this time? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to include also those variables that you think are important and repeat the same step again. Observe what happen to the performances when you add more and more predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the graphs on EDA, it seems that male and female make their decisions quite differently. Try to repeat the above with female_df and male_df and see if the results improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to know how good our prediction performance is, we should at least compare it to the performances of some naive algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of rejection by female in training set is 0.6255690440060698\n",
      "Proportion of rejection by female in test set is 0.6236722306525038\n"
     ]
    }
   ],
   "source": [
    "# what if I just look at the training set and guess the most popular decisions?\n",
    "no_female_train = female_train.query('dec == 0')\n",
    "print('Proportion of rejection by female in training set is {}'\\\n",
    "      .format(yes_female_train.shape[0]/female_train.shape[0]))\n",
    "\n",
    "no_female_test = female_test.query('dec == 0')\n",
    "print('Proportion of rejection by female in test set is {}'\\\n",
    "      .format(yes_female_test.shape[0]/female_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of rejection by male in training set is 0.5162824734723747\n",
      "Proportion of rejection by male in test set is 0.49122807017543857\n"
     ]
    }
   ],
   "source": [
    "# what if I just look at the training set and guess the most popular decisions?\n",
    "no_male_train = male_train.query('dec == 0')\n",
    "print('Proportion of rejection by male in training set is {}'\\\n",
    "      .format(yes_male_train.shape[0]/male_train.shape[0]))\n",
    "\n",
    "no_male_test = male_test.query('dec == 0')\n",
    "print('Proportion of rejection by male in test set is {}'\\\n",
    "      .format(yes_male_test.shape[0]/male_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set performance is 0.7532846715328467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weizhen/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# what if I simply do a cut off at attr_partner and base my decision on that? (refer to graphs plotted in EDA)\n",
    "male_test['attr_cut_predict'] = (male_test['attr_partner']>=7)\n",
    "print('male test set performance is {}'\\\n",
    "      .format((male_test['attr_cut_predict'] == male_test['dec']).sum()/male_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female test set performance is 0.7329286798179059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weizhen/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "female_test['attr_cut_predict'] = (female_test['attr_partner']>=8)\n",
    "print('female test set performance is {}'\\\n",
    "      .format((female_test['attr_cut_predict'] == female_test['dec']).sum()/female_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree model allows combination of factors (as opposed to logistic regression model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(min_impurity_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set performance is 0.8794933879679643\n",
      "test set performance is 0.7267311988086373\n"
     ]
    }
   ],
   "source": [
    "predictors = ['age','date','int_corr',\n",
    " 'samerace',\n",
    " 'sinc_partner',\n",
    " 'intel_partner',\n",
    " 'fun_partner',\n",
    " 'amb_partner',\n",
    " 'shar_partner','prob', 'attr_partner','attr_want',\n",
    " 'sinc_want',\n",
    " 'intel_want',\n",
    " 'fun_want',\n",
    " 'amb_want',\n",
    " 'shar_want']\n",
    "dt_model = dt.fit(train[predictors].values, train['dec'].values)\n",
    "print('training set performance is {}'.format(dt_model.score(train[predictors].values, train['dec'].values)))\n",
    "print('test set performance is {}'.format(dt_model.score(test[predictors].values, test['dec'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02697355,  0.01483152,  0.04568818,  0.00881074,  0.01654893,\n",
       "        0.01588581,  0.03525998,  0.02560737,  0.11024847,  0.05146821,\n",
       "        0.42485679,  0.05590846,  0.03568541,  0.04481684,  0.03602042,\n",
       "        0.02073288,  0.03065645])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set performance is 0.987707208046191\n",
      "test set performance is 0.7580044676098288\n"
     ]
    }
   ],
   "source": [
    "predictors = ['age','date','int_corr',\n",
    " 'samerace',\n",
    " 'sinc_partner',\n",
    " 'intel_partner',\n",
    " 'fun_partner',\n",
    " 'amb_partner',\n",
    " 'shar_partner','prob', 'attr_partner','attr_want',\n",
    " 'sinc_want',\n",
    " 'intel_want',\n",
    " 'fun_want',\n",
    " 'amb_want',\n",
    " 'shar_want']\n",
    "rf_model = rf.fit(train[predictors].values, train['dec'].values)\n",
    "print('training set performance is {}'.format(rf_model.score(train[predictors].values, train['dec'].values)))\n",
    "print('test set performance is {}'.format(rf_model.score(test[predictors].values, test['dec'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier(max_depth=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set performance is 0.979698267833861\n",
      "test set performance is 0.7944899478778853\n"
     ]
    }
   ],
   "source": [
    "predictors = ['age','date','int_corr',\n",
    " 'samerace',\n",
    " 'sinc_partner',\n",
    " 'intel_partner',\n",
    " 'fun_partner',\n",
    " 'amb_partner',\n",
    " 'shar_partner','prob', 'attr_partner','attr_want',\n",
    " 'sinc_want',\n",
    " 'intel_want',\n",
    " 'fun_want',\n",
    " 'amb_want',\n",
    " 'shar_want']\n",
    "gb_model = gb.fit(train[predictors].values, train['dec'].values)\n",
    "print('training set performance is {}'.format(gb_model.score(train[predictors].values, train['dec'].values)))\n",
    "print('test set performance is {}'.format(gb_model.score(test[predictors].values, test['dec'].values)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
